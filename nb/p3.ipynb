{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943e07ce-231f-44b9-ae8d-67a6499c7a6a",
   "metadata": {},
   "source": [
    "## Part 1 Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059ef549-d3fe-4951-9c13-b35745769500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b192c5a-8d91-402d-bb83-b25b8ff6b373",
   "metadata": {},
   "source": [
    "#### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ee76ac-e93a-4eee-bc13-0929e090ef32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-20 19:16:19--  https://pages.cs.wisc.edu/~harter/cs639/data/hdma-wi-2021.csv\n",
      "Resolving pages.cs.wisc.edu (pages.cs.wisc.edu)... 128.105.7.9\n",
      "Connecting to pages.cs.wisc.edu (pages.cs.wisc.edu)|128.105.7.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 174944099 (167M) [text/csv]\n",
      "Saving to: ‘hdma-wi-2021.csv’\n",
      "\n",
      "hdma-wi-2021.csv    100%[===================>] 166.84M  51.7MB/s    in 3.5s    \n",
      "\n",
      "2023-03-20 19:16:23 (48.1 MB/s) - ‘hdma-wi-2021.csv’ saved [174944099/174944099]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget \"https://pages.cs.wisc.edu/~harter/cs639/data/hdma-wi-2021.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbce17-7797-42b1-812d-51e0129658d1",
   "metadata": {},
   "source": [
    "#### Inserting data into our hdfs cluster single.csv without replication and double.csv with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9adba678-d2ff-4367-937e-7fd1781c2c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=1 -cp hdma-wi-2021.csv hdfs://main:9000/single.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcf3252-5cd2-4edc-8911-bd191ba73dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=2 -cp hdma-wi-2021.csv hdfs://main:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13575c40-5dc1-4ab6-9a32-2fb7562d9c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.8 M  333.7 M  hdfs://main:9000/double.csv\n",
      "166.8 M  166.8 M  hdfs://main:9000/single.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h hdfs://main:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f70c14-ac01-422e-b343-82fe098b736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\"http://main:9870/webhdfs/v1/single.csv?op=OPEN&offset=100\", allow_redirects=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a59304-3a01-4319-82ca-06453df59a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Mon, 20 Mar 2023 20:16:38 GMT, Mon, 20 Mar 2023 20:16:40 GMT', 'Cache-Control': 'no-cache', 'Expires': 'Mon, 20 Mar 2023 20:16:40 GMT', 'Pragma': 'no-cache', 'X-Content-Type-Options': 'nosniff', 'X-FRAME-OPTIONS': 'SAMEORIGIN', 'X-XSS-Protection': '1; mode=block', 'Location': 'http://a85252eaac89:9864/webhdfs/v1/single.csv?op=OPEN&namenoderpcaddress=main:9000&offset=100', 'Content-Type': 'application/octet-stream', 'Content-Length': '0'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eac1524-fb87-4604-a95b-5072460c13b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://a85252eaac89:9864/webhdfs/v1/single.csv?op=OPEN&namenoderpcaddress=main:9000&offset=100'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.headers['Location']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4122bc-0feb-4932-97cb-100e74c01e34",
   "metadata": {},
   "source": [
    "## Part2 : Block Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ece3f2-9eaa-45d1-9ca3-d43c61af8256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://a85252eaac89:9864/webhdfs/v1/single.csv': 79,\n",
       " 'http://3034ad7193b6:9864/webhdfs/v1/single.csv': 88}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb = 1048576\n",
    "nodeDict = {}\n",
    "\n",
    "#looping over offsets increasing my 1 MB each time till 167MB which is the file size\n",
    "for i in range(167):\n",
    "    response = requests.get(f\"http://main:9870/webhdfs/v1/single.csv?op=OPEN&offset={i*mb}\", allow_redirects=False)\n",
    "    #only taking node adress which is up until the question mark\n",
    "    qm = response.headers['Location'].index('?')\n",
    "    node = response.headers['Location'][0:qm]\n",
    "    #incrementing node value based if it has current block\n",
    "    if node in nodeDict.keys():\n",
    "        nodeDict[node] += 1\n",
    "    else:\n",
    "        nodeDict[node] = 1\n",
    "        \n",
    "nodeDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008bd53b-5ba6-4bbb-8607-1ce44b369120",
   "metadata": {},
   "source": [
    "## Part3: Reading the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d11b8d-b281-42ec-8b0d-fbf937ff1404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174944099"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = requests.get(\"http://main:9870/webhdfs/v1/single.csv?op=GETFILESTATUS\")\n",
    "status = status.json()\n",
    "status['FileStatus']['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5846918-ba17-42e4-8ac3-62cda3fa5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class hdfsFile(io.RawIOBase):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.offset = 0\n",
    "        status = requests.get(f\"http://main:9870/webhdfs/v1/{self.path}?op=GETFILESTATUS\")\n",
    "        status = status.json()\n",
    "        self.length = int(status['FileStatus']['length'])\n",
    "\n",
    "    def readable(self):\n",
    "        return True\n",
    "\n",
    "    def readinto(self, b):\n",
    "        # If at end of file, return 0\n",
    "        if self.offset >= self.length:\n",
    "            return 0\n",
    "        \n",
    "        # determine how much data to request from HDFS\n",
    "        remaining_bytes = self.length - self.offset\n",
    "        toRead = min(len(b), remaining_bytes)\n",
    "        \n",
    "        # request data from HDFS\n",
    "        res = requests.get(f\"http://main:9870/webhdfs/v1/{self.path}?op=OPEN&offset={self.offset}&length={toRead}\")\n",
    "        \n",
    "        if res.status_code == 200:\n",
    "            data = res.content\n",
    "\n",
    "            #adding to buffer\n",
    "            b[0:len(data)] = data\n",
    "            # update offset\n",
    "            self.offset += toRead\n",
    "            \n",
    "            toRet = len(data)\n",
    "        \n",
    "        #PART4 CODE: replace missing blocks with new line (our block size is of 1MB (1048576 bytes)\n",
    "        else:\n",
    "            nl = bytearray(b'\\n')\n",
    "            b[0:len(nl)] = nl\n",
    "            self.offset = (self.offset//1048576 + 1) * 1048576\n",
    "            toRet = 1\n",
    "        # return number of bytes read\n",
    "        return toRet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b4f5e-f5ab-4d2a-9ec7-453b054e5d91",
   "metadata": {},
   "source": [
    "### Counting Single and Multiple Family mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef73546-52e8-479c-83c8-0c7e175f495b",
   "metadata": {},
   "source": [
    "#### trying with buffer size 1000000 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beaf49d2-675d-4b22-b853-f62e0298b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv\n",
      "Single Family: 444874\n",
      "Multi Family: 2493\n",
      "Seconds: 15.973117589950562\n"
     ]
    }
   ],
   "source": [
    "singleFam = 0\n",
    "multFam = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"), 1000000):\n",
    "    line = str(line, \"utf-8\")\n",
    "    if \"Single Family\" in line:\n",
    "        singleFam += 1\n",
    "    if \"Multifamily\" in line:\n",
    "        multFam += 1\n",
    "        \n",
    "t1 = time.time()\n",
    "diff = t1 - t0\n",
    "\n",
    "print(\"Counts from single.csv\")\n",
    "print(\"Single Family:\", singleFam)\n",
    "print(\"Multi Family:\" , multFam)\n",
    "print(\"Seconds:\", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63681e-9225-4e83-b230-6570379db03b",
   "metadata": {},
   "source": [
    "#### trying with buffer size 100000000 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac265259-68d8-4690-ad64-bdee46dc7eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv\n",
      "Single Family: 444874\n",
      "Multi Family: 2493\n",
      "Seconds: 2.7151737213134766\n"
     ]
    }
   ],
   "source": [
    "singleFam = 0\n",
    "multFam = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"), 100000000):\n",
    "    line = str(line, \"utf-8\")\n",
    "    if \"Single Family\" in line:\n",
    "        singleFam += 1\n",
    "    if \"Multifamily\" in line:\n",
    "        multFam += 1\n",
    "        \n",
    "t1 = time.time()\n",
    "diff = t1 - t0\n",
    "\n",
    "print(\"Counts from single.csv\")\n",
    "print(\"Single Family:\", singleFam)\n",
    "print(\"Multi Family:\" , multFam)\n",
    "print(\"Seconds:\", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b4695-901b-45a4-ac54-00a33e5b1b42",
   "metadata": {},
   "source": [
    "#### we can see that increasing the buffer size speeds up our read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f7dde-35d6-4453-95c3-105551306131",
   "metadata": {},
   "source": [
    "## Part4: Disaster Strikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4cbd6-0c95-48fa-8cb3-5654002f15f4",
   "metadata": {},
   "source": [
    "#### this next part of code is ran after killing one of our workers aka datanode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c299954-352f-412f-9da9-c07a94ef81b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 51642105856 (48.10 GB)\n",
      "Present Capacity: 33098988668 (30.83 GB)\n",
      "DFS Remaining: 32570003456 (30.33 GB)\n",
      "DFS Used: 528985212 (504.48 MB)\n",
      "DFS Used%: 1.60%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.18.0.4:9866 (project-3-cat-mongers-worker-1.cs544net)\n",
      "Hostname: 3034ad7193b6\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 269332796 (256.86 MB)\n",
      "Non DFS Used: 9249955524 (8.61 GB)\n",
      "DFS Remaining: 16284987392 (15.17 GB)\n",
      "DFS Used%: 1.04%\n",
      "DFS Remaining%: 63.07%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Mon Mar 20 20:23:26 GMT 2023\n",
      "Last Block Report: Mon Mar 20 20:14:23 GMT 2023\n",
      "Num of Blocks: 255\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.18.0.2:9866 (172.18.0.2)\n",
      "Hostname: a85252eaac89\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 259652416 (247.62 MB)\n",
      "Non DFS Used: 9259607232 (8.62 GB)\n",
      "DFS Remaining: 16285016064 (15.17 GB)\n",
      "DFS Used%: 1.01%\n",
      "DFS Remaining%: 63.07%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Mon Mar 20 20:21:56 GMT 2023\n",
      "Last Block Report: Mon Mar 20 20:14:23 GMT 2023\n",
      "Num of Blocks: 246\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -fs hdfs://main:9000/ -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "256d7a63-e840-469f-a3e2-083b1abbf507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv\n",
      "Single Family: 234461\n",
      "Multi Family: 1220\n"
     ]
    }
   ],
   "source": [
    "singleFam = 0\n",
    "multFam = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"), 1048576):\n",
    "    line = str(line, \"utf-8\")\n",
    "    if \"Single Family\" in line:\n",
    "        singleFam += 1\n",
    "    if \"Multifamily\" in line:\n",
    "        multFam += 1\n",
    "        \n",
    "t1 = time.time()\n",
    "diff = t1 - t0\n",
    "\n",
    "print(\"Counts from single.csv\")\n",
    "print(\"Single Family:\", singleFam)\n",
    "print(\"Multi Family:\" , multFam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f0bb3-22a4-410c-beb5-846c32680d46",
   "metadata": {},
   "source": [
    "We see that single.csv had no replication so has lost data indicated by lesser lines when than when we ran with both nodes running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7afea03f-e446-4b40-a04e-9e5000ff6ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from double.csv\n",
      "Single Family: 444874\n",
      "Multi Family: 2493\n"
     ]
    }
   ],
   "source": [
    "singleFam = 0\n",
    "multFam = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"double.csv\"), 1048576):\n",
    "    line = str(line, \"utf-8\")\n",
    "    if \"Single Family\" in line:\n",
    "        singleFam += 1\n",
    "    if \"Multifamily\" in line:\n",
    "        multFam += 1\n",
    "        \n",
    "t1 = time.time()\n",
    "diff = t1 - t0\n",
    "\n",
    "print(\"Counts from double.csv\")\n",
    "print(\"Single Family:\", singleFam)\n",
    "print(\"Multi Family:\" , multFam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dfcca7-1b97-4ad3-b5fb-44d5040be89a",
   "metadata": {},
   "source": [
    "However, double.csv has been replicated so suffers no dataloss on death of one of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947d3ab-be43-4101-8aba-a7a44c83c4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
